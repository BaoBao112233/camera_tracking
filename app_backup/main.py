import cv2
import numpy as np
import time
import json
import os
import threading
from typing import Generator
from collections import OrderedDict
from fastapi import FastAPI, Response, Request
from fastapi.responses import StreamingResponse, HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from contextlib import asynccontextmanager
import uvicorn
from ultralytics import YOLO
from dotenv import load_dotenv
import torch
torch.serialization.add_safe_globals([__import__("ultralytics").nn.tasks.DetectionModel])


# Load environment variables
load_dotenv()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Qu·∫£n l√Ω lifecycle c·ªßa ·ª©ng d·ª•ng
    """
    global CONFIDENCE_THRESHOLD, tracker, threaded_camera
    
    # Startup
    initialize_detector()
    initialize_pose_detector()
    
    # In th√¥ng tin c·∫•u h√¨nh
    print(f"üöÄ Server ƒëang kh·ªüi ƒë·ªông...")
    print(f"üìπ RTSP URL: {RTSP_URL}")
    server_config = config.get("server_config", {"host": "0.0.0.0", "port": 8000})
    print(f"üåê Server: http://{server_config['host']}:{server_config['port']}")
    print(f"üéØ Confidence Threshold: {CONFIDENCE_THRESHOLD}")
    print(f"üìä Tracker Config: {config.get('tracker_config', {})}")
    print(f"ü§∏ Fall Detection: {'Enabled' if yolo_pose_model else 'Disabled'}")
    
    yield
    
    # Shutdown
    if threaded_camera:
        threaded_camera.stop()
    print("üõë Server ƒë√£ t·∫Øt")

app = FastAPI(title="Camera Tracking API", description="RTSP streaming v·ªõi ƒë·∫øm ng∆∞·ªùi ra v√†o", lifespan=lifespan)

# Thi·∫øt l·∫≠p templates
templates = Jinja2Templates(directory="templates")

# Load c·∫•u h√¨nh t·ª´ file JSON
def load_config():
    return {
        "rtsp_url": os.getenv("RTSP_URL"),
        "confidence_threshold": 0.5,
        "tracker_config": {
            "max_disappeared": 100,
            "max_distance": 100
        },
        "server_config": {
            "host": "0.0.0.0",
            "port": 8000
        },
        "model_paths": {
            "yolo_model": "detector/yolov8n.pt"
        },
        "video_config": {
            "fps_limit": 15,
            "frame_skip": 1,
            "buffer_size": 1,
            "reconnect_delay": 1
        },
        "detection_config": {
            "person_class_id": 0,
            "input_size": [640, 640],
            "model_type": "yolov8"
        },
        "ui_config": {
            "line_color": [0, 255, 255],
            "line_thickness": 2,
            "bbox_color": [0, 255, 0],
            "bbox_thickness": 2,
            "text_color": [0, 255, 0],
            "text_scale": 0.5,
            "text_thickness": 2
        },
    }

# Load c·∫•u h√¨nh
config = load_config()

# C·∫•u h√¨nh t·ª´ file JSON
RTSP_URL = config["rtsp_url"]
YOLO_MODEL_PATH = config["model_paths"]["yolo_model"]
CONFIDENCE_THRESHOLD = config["confidence_threshold"]

# Kh·ªüi t·∫°o detector
yolo_model = None

# Bi·∫øn ƒë·∫øm ng∆∞·ªùi
people_count_in = 0
people_count_out = 0
total_people = 0

# L∆∞u tr·ªØ l·ªãch s·ª≠ v·ªã tr√≠ c·ªßa objects ƒë·ªÉ theo d√µi h∆∞·ªõng di chuy·ªÉn (t·ªëi ƒëa 30 frame)
previous_positions = {}

# Bi·∫øn cho fall detection
fall_detection_active = False
total_falls = 0
people_detected_fall = 0
last_fall_time = None
last_fall_tracker_id = None  # L∆∞u tracker_id c·ªßa s·ª± ki·ªán ng√£ g·∫ßn ƒë√¢y nh·∫•t
fall_sensitivity = "medium"  # low, medium, high
fall_confidence_threshold = 0.1
yolo_pose_model = None

# Tracking fall events ƒë·ªÉ tr√°nh ƒë·∫øm tr√πng l·∫∑p
fall_tracker = {}  # {tracker_id: {'is_fallen': bool, 'fall_start_time': float, 'fall_counted': bool}}
fall_cooldown_time = 3.0  # Th·ªùi gian cooldown gi·ªØa c√°c l·∫ßn ƒë·∫øm ng√£ (gi√¢y)
max_fall_duration = 10.0  # Th·ªùi gian t·ªëi ƒëa m·ªôt l·∫ßn ng√£ (gi√¢y)

# Threaded Camera class ƒë·ªÉ gi·∫£m delay
class ThreadedCamera:
    def __init__(self, src=0):
        """
        Kh·ªüi t·∫°o threaded camera ƒë·ªÉ gi·∫£m delay
        """
        print(f"üîÑ ƒêang kh·ªüi t·∫°o camera v·ªõi URL: {src}")
        self.capture = cv2.VideoCapture(src)
        
        # Ki·ªÉm tra camera c√≥ m·ªü ƒë∆∞·ª£c kh√¥ng
        if not self.capture.isOpened():
            print(f"‚ùå Kh√¥ng th·ªÉ m·ªü camera v·ªõi URL: {src}")
            raise Exception(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi camera: {src}")
        
        # C·∫•u h√¨nh camera
        self.capture.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        self.capture.set(cv2.CAP_PROP_FPS, 60)
        
        # Kh·ªüi t·∫°o bi·∫øn
        self.frame = None
        self.status = False
        self.running = True
        self.frame_count = 0
        
        # B·∫Øt ƒë·∫ßu thread
        self.thread = threading.Thread(target=self.update)
        self.thread.daemon = True
        self.thread.start()
        
        print(f"‚úÖ Camera ƒë√£ kh·ªüi t·∫°o th√†nh c√¥ng")
    
    def update(self):
        """
        C·∫≠p nh·∫≠t frame li√™n t·ª•c trong thread ri√™ng
        """
        try:
            while hasattr(self, 'running') and self.running:
                if self.capture.isOpened():
                    (self.status, self.frame) = self.capture.read()
                    if self.status:
                        self.frame_count += 1
                        if self.frame_count % 100 == 0:  # Log m·ªói 100 frames
                            print(f"üìπ ƒê√£ ƒë·ªçc {self.frame_count} frames")
                    else:
                        print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c frame, status: {self.status}")
                else:
                    print("‚ùå Camera kh√¥ng m·ªü ƒë∆∞·ª£c")
                    break
                time.sleep(0.01)
        except Exception as e:
            print(f"‚ùå L·ªói trong ThreadedCamera.update: {e}")
    
    def read(self):
        """
        ƒê·ªçc frame hi·ªán t·∫°i
        """
        if self.frame is None:
            print("‚ö†Ô∏è Frame l√† None")
        return self.status, self.frame
    
    def stop(self):
        """
        D·ª´ng camera thread
        """
        try:
            self.running = False
            if hasattr(self, 'thread') and self.thread.is_alive():
                self.thread.join(timeout=2)
            if hasattr(self, 'capture') and self.capture.isOpened():
                self.capture.release()
        except Exception as e:
            print(f"L·ªói khi d·ª´ng ThreadedCamera: {e}")

# Kh·ªüi t·∫°o threaded camera
threaded_camera = None

# Centroid tracker c·∫£i ti·∫øn v·ªõi Kalman Filter
class CentroidTracker:
    def __init__(self, max_disappeared=100, max_distance=100):
        """
        Kh·ªüi t·∫°o centroid tracker c·∫£i ti·∫øn v·ªõi Kalman Filter
        """
        self.next_object_id = 0
        self.objects = OrderedDict()  # V·ªã tr√≠ hi·ªán t·∫°i
        self.disappeared = OrderedDict()
        self.velocities = OrderedDict()  # V·∫≠n t·ªëc c·ªßa objects
        self.bboxes = OrderedDict()  # L∆∞u bounding boxes
        self.max_disappeared = max_disappeared
        self.max_distance = max_distance

    def register(self, centroid, bbox=None):
        """
        ƒêƒÉng k√Ω object m·ªõi v·ªõi centroid v√† bbox
        """
        self.objects[self.next_object_id] = centroid
        self.disappeared[self.next_object_id] = 0
        self.velocities[self.next_object_id] = np.array([0.0, 0.0])  # V·∫≠n t·ªëc ban ƒë·∫ßu
        self.bboxes[self.next_object_id] = bbox if bbox is not None else (0, 0, 0, 0)
        self.next_object_id += 1

    def deregister(self, object_id):
        """
        H·ªßy ƒëƒÉng k√Ω object v√† x√≥a t·∫•t c·∫£ th√¥ng tin li√™n quan
        """
        del self.objects[object_id]
        del self.disappeared[object_id]
        del self.velocities[object_id]
        del self.bboxes[object_id]

    def calculate_iou(self, box1, box2):
        """
        T√≠nh Intersection over Union (IoU) gi·ªØa hai bounding boxes
        """
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # T√≠nh di·ªán t√≠ch giao nhau
        x1_inter = max(x1_1, x1_2)
        y1_inter = max(y1_1, y1_2)
        x2_inter = min(x2_1, x2_2)
        y2_inter = min(y2_1, y2_2)
        
        if x2_inter <= x1_inter or y2_inter <= y1_inter:
            return 0.0
        
        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
        
        # T√≠nh di·ªán t√≠ch h·ª£p nh·∫•t
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = area1 + area2 - inter_area
        
        return inter_area / union_area if union_area > 0 else 0.0

    def predict_position(self, object_id):
        """
        D·ª± ƒëo√°n v·ªã tr√≠ ti·∫øp theo d·ª±a tr√™n v·∫≠n t·ªëc
        """
        if object_id in self.objects and object_id in self.velocities:
            current_pos = np.array(self.objects[object_id])
            velocity = self.velocities[object_id]
            predicted_pos = current_pos + velocity
            return predicted_pos.astype(int)
        return None

    def update(self, rects):
        """
        C·∫≠p nh·∫≠t tracker v·ªõi danh s√°ch bounding boxes - c·∫£i ti·∫øn v·ªõi IoU v√† d·ª± ƒëo√°n
        """
        if len(rects) == 0:
            # D·ª± ƒëo√°n v·ªã tr√≠ cho c√°c objects b·ªã m·∫•t
            for object_id in list(self.disappeared.keys()):
                predicted_pos = self.predict_position(object_id)
                if predicted_pos is not None:
                    self.objects[object_id] = predicted_pos
                
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
            return self.objects

        # T√≠nh centroids v√† l∆∞u bounding boxes
        input_centroids = np.zeros((len(rects), 2), dtype="int")
        input_bboxes = []
        for (i, (start_x, start_y, end_x, end_y)) in enumerate(rects):
            cx = int((start_x + end_x) / 2.0)
            cy = int((start_y + end_y) / 2.0)
            input_centroids[i] = (cx, cy)
            input_bboxes.append((start_x, start_y, end_x, end_y))

        if len(self.objects) == 0:
            # ƒêƒÉng k√Ω t·∫•t c·∫£ objects m·ªõi
            for i in range(len(input_centroids)):
                self.register(input_centroids[i], input_bboxes[i])
        else:
            # T√≠nh ma tr·∫≠n kho·∫£ng c√°ch v√† IoU
            object_centroids = list(self.objects.values())
            object_ids = list(self.objects.keys())
            
            # Ma tr·∫≠n kho·∫£ng c√°ch centroid
            D = np.linalg.norm(np.array(object_centroids)[:, np.newaxis] - input_centroids, axis=2)
            
            # Ma tr·∫≠n IoU
            IoU_matrix = np.zeros((len(object_ids), len(input_bboxes)))
            for i, obj_id in enumerate(object_ids):
                for j, input_bbox in enumerate(input_bboxes):
                    if obj_id in self.bboxes:
                        iou = self.calculate_iou(self.bboxes[obj_id], input_bbox)
                        IoU_matrix[i, j] = iou
            
            # K·∫øt h·ª£p distance v√† IoU ƒë·ªÉ t·∫°o cost matrix
            # Normalize distance matrix
            D_norm = D / (self.max_distance + 1e-6)
            # IoU cost (1 - IoU ƒë·ªÉ c√≥ cost th·∫•p h∆°n cho IoU cao h∆°n)
            IoU_cost = 1 - IoU_matrix
            
            # Combined cost: 70% distance + 30% IoU
            cost_matrix = 0.7 * D_norm + 0.3 * IoU_cost
            
            # Hungarian algorithm simulation v·ªõi greedy approach
            rows = cost_matrix.min(axis=1).argsort()
            cols = cost_matrix.argmin(axis=1)[rows]

            used_row_indices = set()
            used_col_indices = set()

            for (row, col) in zip(rows, cols):
                if row in used_row_indices or col in used_col_indices:
                    continue

                # Ki·ªÉm tra c·∫£ distance v√† IoU thresholds
                if D[row, col] > self.max_distance and IoU_matrix[row, col] < 0.3:
                    continue

                object_id = object_ids[row]
                old_centroid = self.objects[object_id]
                new_centroid = input_centroids[col]
                
                # C·∫≠p nh·∫≠t v·∫≠n t·ªëc
                velocity = np.array(new_centroid) - np.array(old_centroid)
                self.velocities[object_id] = 0.7 * self.velocities[object_id] + 0.3 * velocity
                
                # C·∫≠p nh·∫≠t v·ªã tr√≠ v√† bbox
                self.objects[object_id] = new_centroid
                self.bboxes[object_id] = input_bboxes[col]
                self.disappeared[object_id] = 0

                used_row_indices.add(row)
                used_col_indices.add(col)

            unused_row_indices = set(range(0, len(object_ids))).difference(used_row_indices)
            unused_col_indices = set(range(0, len(input_centroids))).difference(used_col_indices)

            # X·ª≠ l√Ω objects kh√¥ng ƒë∆∞·ª£c match
            for row in unused_row_indices:
                object_id = object_ids[row]
                # D·ª± ƒëo√°n v·ªã tr√≠ m·ªõi
                predicted_pos = self.predict_position(object_id)
                if predicted_pos is not None:
                    self.objects[object_id] = predicted_pos
                
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
            
            # ƒêƒÉng k√Ω detections m·ªõi
            for col in unused_col_indices:
                self.register(input_centroids[col], input_bboxes[col])

        return self.objects

# Kh·ªüi t·∫°o tracker v·ªõi c·∫•u h√¨nh
tracker_config = config.get("tracker_config", {"max_disappeared": 50, "max_distance": 50})
tracker = CentroidTracker(
    max_disappeared=tracker_config["max_disappeared"],
    max_distance=tracker_config["max_distance"]
)

# Pose tracker ƒë·ªÉ tracking ng∆∞·ªùi v·ªõi pose detection
pose_tracker = CentroidTracker(max_disappeared=30, max_distance=80)

def initialize_detector():
    """
    Kh·ªüi t·∫°o YOLOv8n detector
    """
    global yolo_model
    try:
        yolo_model = YOLO(YOLO_MODEL_PATH)
        print(f"YOLOv8n detector ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng t·ª´ {YOLO_MODEL_PATH}")
    except Exception as e:
        print(f"L·ªói kh·ªüi t·∫°o YOLOv8n detector: {e}")
        yolo_model = None

def initialize_pose_detector():
    """
    Kh·ªüi t·∫°o YOLO pose detector cho fall detection
    """
    global yolo_pose_model
    try:
        pose_model_path = "detector/yolov8s-pose.pt"
        print(f"üîÑ ƒêang t·∫£i YOLO pose model t·ª´: {pose_model_path}")
        yolo_pose_model = YOLO(pose_model_path)
        print(f"‚úÖ YOLO pose model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i YOLO pose model: {e}")
        yolo_pose_model = None

def detect_people(frame):
    """
    Ph√°t hi·ªán ng∆∞·ªùi trong frame s·ª≠ d·ª•ng YOLOv8n
    Returns:
        List of bounding boxes cho ng∆∞·ªùi ƒë∆∞·ª£c ph√°t hi·ªán
    """
    if yolo_model is None:
        print("‚ö†Ô∏è YOLOv8n detector ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o (yolo_model is None)")
        return []
    
    try:
        # Ch·∫°y inference v·ªõi YOLOv8
        results = yolo_model(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)
        
        rects = []
        detection_config = config.get("detection_config", {})
        person_class_id = detection_config.get("person_class_id", 0)  # Class 0 l√† person trong YOLO
        
        # X·ª≠ l√Ω k·∫øt qu·∫£ detection
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    # L·∫•y class id v√† confidence
                    cls = int(box.cls[0])
                    conf = float(box.conf[0])
                    
                    # Ch·ªâ l·∫•y detection c·ªßa ng∆∞·ªùi (class 0)
                    if cls == person_class_id and conf > CONFIDENCE_THRESHOLD:
                        # L·∫•y t·ªça ƒë·ªô bounding box
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        rects.append((int(x1), int(y1), int(x2), int(y2)))
        
        return rects
        
    except Exception as e:
        print(f"‚ùå L·ªói trong detect_people v·ªõi YOLOv8: {e}")
        return []

def detect_poses_and_falls(frame):
    """
    Ph√°t hi·ªán pose v√† ng∆∞·ªùi ng√£ s·ª≠ d·ª•ng YOLO pose detection v·ªõi centroid tracking ƒë·ªÉ tr√°nh ƒë·∫øm tr√πng l·∫∑p
    """
    global yolo_pose_model, total_falls, people_detected_fall, last_fall_time, fall_tracker, pose_tracker
    
    if yolo_pose_model is None:
        return [], False, None
    
    try:
        # Ch·∫°y pose detection
        results = yolo_pose_model(frame, conf=fall_confidence_threshold)
        
        poses_data = []
        fall_detected = False
        fall_person_bbox = None
        current_time = time.time()
        
        # Cleanup old fall tracker entries
        cleanup_fall_tracker(current_time)
        
        # Thu th·∫≠p t·∫•t c·∫£ bounding boxes v√† pose results ƒë·ªÉ tracking
        detected_boxes = []
        pose_results = []
        
        for result in results:
            boxes = result.boxes
            keypoints = result.keypoints
            
            if boxes is not None and keypoints is not None:
                for i, (box, kpts) in enumerate(zip(boxes, keypoints)):
                    # L·∫•y bounding box
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
                    confidence = box.conf[0].cpu().numpy()
                    
                    if confidence >= fall_confidence_threshold:
                        # L·∫•y keypoints (17 ƒëi·ªÉm cho COCO format)
                        kpts_xy = kpts.xy[0].cpu().numpy()  # Shape: (17, 2)
                        kpts_conf = kpts.conf[0].cpu().numpy()  # Shape: (17,)
                        
                        # Ph√¢n t√≠ch t∆∞ th·∫ø ƒë·ªÉ ph√°t hi·ªán ng√£
                        is_fallen = analyze_pose_for_fall(kpts_xy, kpts_conf, (x1, y1, x2, y2))
                        
                        detected_boxes.append((x1, y1, x2, y2))
                        pose_results.append({
                            'bbox': (x1, y1, x2, y2),
                            'keypoints': kpts_xy,
                            'keypoints_conf': kpts_conf,
                            'is_fallen': is_fallen,
                            'confidence': float(confidence)
                        })
        
        # C·∫≠p nh·∫≠t tracker v·ªõi c√°c bounding boxes
        tracked_objects = pose_tracker.update(detected_boxes)
        
        # K·∫øt h·ª£p tracking results v·ªõi pose results
        for tracker_id, centroid in tracked_objects.items():
            # T√¨m pose result t∆∞∆°ng ·ª©ng v·ªõi tracker n√†y
            best_match = None
            min_distance = float('inf')
            
            for pose_result in pose_results:
                bbox = pose_result['bbox']
                bbox_centroid = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
                distance = ((centroid[0] - bbox_centroid[0])**2 + (centroid[1] - bbox_centroid[1])**2)**0.5
                
                if distance < min_distance:
                    min_distance = distance
                    best_match = pose_result
            
            if best_match and min_distance < 50:  # Ng∆∞·ª°ng kho·∫£ng c√°ch ƒë·ªÉ match
                # X·ª≠ l√Ω fall tracking v·ªõi tracker_id
                fall_event_detected = process_fall_tracking(tracker_id, best_match['is_fallen'], current_time)
                
                pose_info = {
                    'bbox': best_match['bbox'],
                    'keypoints': [{'x': float(kpt[0]), 'y': float(kpt[1]), 'confidence': float(conf)} 
                                for kpt, conf in zip(best_match['keypoints'], best_match['keypoints_conf'])],
                    'is_fallen': best_match['is_fallen'],
                    'confidence': best_match['confidence'],
                    'tracker_id': tracker_id,
                    'centroid': centroid
                }
                poses_data.append(pose_info)
                
                if fall_event_detected:
                    fall_detected = True
                    fall_person_bbox = best_match['bbox']
                    last_fall_time = current_time
                    print(f"üö® PH√ÅT HI·ªÜN NG∆Ø·ªúI NG√É M·ªöI! Tracker ID: {tracker_id}, T·ªïng s·ªë l·∫ßn ng√£: {total_falls}")
        
        people_detected_fall = len(poses_data)
        return poses_data, fall_detected, fall_person_bbox
        
    except Exception as e:
        print(f"‚ùå L·ªói trong detect_poses_and_falls: {e}")
        return [], False, None

def cleanup_fall_tracker(current_time):
    """
    D·ªçn d·∫πp c√°c entry c≈© trong fall_tracker
    """
    global fall_tracker
    
    to_remove = []
    for tracker_id, data in fall_tracker.items():
        # X√≥a entry n·∫øu qu√° l√¢u kh√¥ng c·∫≠p nh·∫≠t
        if current_time - data.get('last_update', 0) > max_fall_duration:
            to_remove.append(tracker_id)
    
    for tracker_id in to_remove:
        del fall_tracker[tracker_id]

def process_fall_tracking(tracker_id, is_fallen, current_time):
    """
    X·ª≠ l√Ω tracking fall events ƒë·ªÉ tr√°nh ƒë·∫øm tr√πng l·∫∑p
    Tr·∫£ v·ªÅ True n·∫øu ƒë√¢y l√† m·ªôt fall event m·ªõi c·∫ßn ƒë·∫øm
    """
    global fall_tracker, total_falls, last_fall_tracker_id
    
    # Kh·ªüi t·∫°o tracking cho tracker m·ªõi
    if tracker_id not in fall_tracker:
        fall_tracker[tracker_id] = {
            'is_fallen': False,
            'fall_start_time': None,
            'fall_counted': False,
            'last_update': current_time
        }
    
    tracker_data = fall_tracker[tracker_id]
    tracker_data['last_update'] = current_time
    
    # N·∫øu ng∆∞·ªùi n√†y ƒëang ng√£
    if is_fallen:
        # N·∫øu ch∆∞a ƒë∆∞·ª£c ƒë√°nh d·∫•u l√† ng√£ tr∆∞·ªõc ƒë√≥
        if not tracker_data['is_fallen']:
            # B·∫Øt ƒë·∫ßu fall event m·ªõi
            tracker_data['is_fallen'] = True
            tracker_data['fall_start_time'] = current_time
            tracker_data['fall_counted'] = False
        
        # Ki·ªÉm tra xem c√≥ n√™n ƒë·∫øm fall n√†y kh√¥ng
        if not tracker_data['fall_counted']:
            # ƒê·∫øm fall n√†y v√† ƒë√°nh d·∫•u ƒë√£ ƒë·∫øm
            tracker_data['fall_counted'] = True
            total_falls += 1
            last_fall_tracker_id = tracker_id  # L∆∞u tracker_id c·ªßa fall event m·ªõi
            return True  # ƒê√¢y l√† fall event m·ªõi
    
    else:
        # Ng∆∞·ªùi kh√¥ng c√≤n ng√£
        if tracker_data['is_fallen']:
            # K·∫øt th√∫c fall event
            tracker_data['is_fallen'] = False
            tracker_data['fall_start_time'] = None
            # Reset fall_counted sau cooldown time
            if tracker_data['fall_counted'] and current_time - tracker_data.get('fall_start_time', 0) > fall_cooldown_time:
                tracker_data['fall_counted'] = False
    
    return False  # Kh√¥ng ph·∫£i fall event m·ªõi

def analyze_pose_for_fall(keypoints, confidences, bbox):
    """
    Ph√¢n t√≠ch keypoints ƒë·ªÉ x√°c ƒë·ªãnh ng∆∞·ªùi c√≥ b·ªã ng√£ kh√¥ng
    """
    try:
        # COCO keypoints indices
        # 0: nose, 1: left_eye, 2: right_eye, 3: left_ear, 4: right_ear
        # 5: left_shoulder, 6: right_shoulder, 7: left_elbow, 8: right_elbow
        # 9: left_wrist, 10: right_wrist, 11: left_hip, 12: right_hip
        # 13: left_knee, 14: right_knee, 15: left_ankle, 16: right_ankle
        
        # L·∫•y c√°c ƒëi·ªÉm quan tr·ªçng
        nose = keypoints[0] if confidences[0] > 0.2 else None
        left_shoulder = keypoints[5] if confidences[5] > 0.2 else None
        right_shoulder = keypoints[6] if confidences[6] > 0.2 else None
        left_hip = keypoints[11] if confidences[11] > 0.2 else None
        right_hip = keypoints[12] if confidences[12] > 0.2 else None
        left_ankle = keypoints[15] if confidences[15] > 0.2 else None
        right_ankle = keypoints[16] if confidences[16] > 0.2 else None
        
        # T√≠nh to√°n c√°c ch·ªâ s·ªë ƒë·ªÉ ph√°t hi·ªán ng√£
        fall_indicators = 0
        total_checks = 0
        
        # 1. Ki·ªÉm tra t·ª∑ l·ªá chi·ªÅu cao/chi·ªÅu r·ªông c·ªßa bounding box
        x1, y1, x2, y2 = bbox
        bbox_width = x2 - x1
        bbox_height = y2 - y1
        aspect_ratio = bbox_width / bbox_height if bbox_height > 0 else 0
        
        # N·∫øu t·ª∑ l·ªá r·ªông/cao > 1.2, c√≥ th·ªÉ l√† ng∆∞·ªùi n·∫±m
        if aspect_ratio > 1.2:
            fall_indicators += 1
        total_checks += 1
        
        # 2. Ki·ªÉm tra v·ªã tr√≠ ƒë·∫ßu so v·ªõi h√¥ng
        if nose is not None and (left_hip is not None or right_hip is not None):
            hip_y = left_hip[1] if left_hip is not None else right_hip[1]
            if right_hip is not None and left_hip is not None:
                hip_y = (left_hip[1] + right_hip[1]) / 2
            
            # N·∫øu ƒë·∫ßu kh√¥ng cao h∆°n h√¥ng nhi·ªÅu (< 20% chi·ªÅu cao bbox)
            head_hip_diff = hip_y - nose[1]
            if head_hip_diff < bbox_height * 0.2:
                fall_indicators += 1
            total_checks += 1
        
        # 3. Ki·ªÉm tra v·ªã tr√≠ vai so v·ªõi h√¥ng
        if (left_shoulder is not None or right_shoulder is not None) and (left_hip is not None or right_hip is not None):
            shoulder_y = left_shoulder[1] if left_shoulder is not None else right_shoulder[1]
            if left_shoulder is not None and right_shoulder is not None:
                shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2
            
            hip_y = left_hip[1] if left_hip is not None else right_hip[1]
            if left_hip is not None and right_hip is not None:
                hip_y = (left_hip[1] + right_hip[1]) / 2
            
            # N·∫øu vai v√† h√¥ng g·∫ßn ngang nhau
            shoulder_hip_diff = abs(shoulder_y - hip_y)
            if shoulder_hip_diff < bbox_height * 0.15:
                fall_indicators += 1
            total_checks += 1
        
        # 4. Ki·ªÉm tra v·ªã tr√≠ ch√¢n so v·ªõi th√¢n
        if (left_ankle is not None or right_ankle is not None) and (left_hip is not None or right_hip is not None):
            ankle_y = left_ankle[1] if left_ankle is not None else right_ankle[1]
            if left_ankle is not None and right_ankle is not None:
                ankle_y = (left_ankle[1] + right_ankle[1]) / 2
            
            hip_y = left_hip[1] if left_hip is not None else right_hip[1]
            if left_hip is not None and right_hip is not None:
                hip_y = (left_hip[1] + right_hip[1]) / 2
            
            # N·∫øu ch√¢n kh√¥ng th·∫•p h∆°n h√¥ng nhi·ªÅu
            ankle_hip_diff = ankle_y - hip_y
            if ankle_hip_diff < bbox_height * 0.3:
                fall_indicators += 1
            total_checks += 1
        
        # ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng theo ƒë·ªô nh·∫°y - c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c
        sensitivity_thresholds = {
            "low": {"aspect_ratio": 2.2, "head_hip_ratio": 0.25, "shoulder_hip_ratio": 0.35, "knee_body_ratio": 0.5},
            "medium": {"aspect_ratio": 1.8, "head_hip_ratio": 0.35, "shoulder_hip_ratio": 0.45, "knee_body_ratio": 0.6},
            "high": {"aspect_ratio": 1.4, "head_hip_ratio": 0.45, "shoulder_hip_ratio": 0.55, "knee_body_ratio": 0.7}
        }
        
        thresholds = sensitivity_thresholds.get(fall_sensitivity, sensitivity_thresholds["medium"])
        
        fall_indicators = 0
        
        # 1. Ki·ªÉm tra t·ª∑ l·ªá khung h√¨nh (ng∆∞·ªùi n·∫±m ngang) - ch·ªâ s·ªë quan tr·ªçng nh·∫•t
        if aspect_ratio > thresholds["aspect_ratio"]:
            fall_indicators += 2  # TƒÉng tr·ªçng s·ªë cho ch·ªâ s·ªë n√†y
        
        # 2. Ki·ªÉm tra v·ªã tr√≠ ƒë·∫ßu so v·ªõi h√¥ng (ƒë·∫ßu th·∫•p h∆°n h√¥ng)
        if nose is not None and (left_hip is not None or right_hip is not None):
            hip_y = left_hip[1] if left_hip is not None else right_hip[1]
            if right_hip is not None and left_hip is not None:
                hip_y = (left_hip[1] + right_hip[1]) / 2
            
            if nose[1] > hip_y:  # Y tƒÉng xu·ªëng d∆∞·ªõi
                head_hip_diff = abs(nose[1] - hip_y) / bbox_height
                if head_hip_diff > thresholds["head_hip_ratio"]:
                    fall_indicators += 1
        
        # 3. Ki·ªÉm tra vai so v·ªõi h√¥ng
        if (left_shoulder is not None or right_shoulder is not None) and (left_hip is not None or right_hip is not None):
            shoulder_y = left_shoulder[1] if left_shoulder is not None else right_shoulder[1]
            if left_shoulder is not None and right_shoulder is not None:
                shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2
            
            hip_y = left_hip[1] if left_hip is not None else right_hip[1]
            if left_hip is not None and right_hip is not None:
                hip_y = (left_hip[1] + right_hip[1]) / 2
            
            if shoulder_y > hip_y:
                shoulder_hip_diff = abs(shoulder_y - hip_y) / bbox_height
                if shoulder_hip_diff > thresholds["shoulder_hip_ratio"]:
                    fall_indicators += 1
        
        # 4. Ki·ªÉm tra ch√¢n so v·ªõi th√¢n
        if (left_ankle is not None or right_ankle is not None) and (left_hip is not None or right_hip is not None):
            ankle_y = left_ankle[1] if left_ankle is not None else right_ankle[1]
            if left_ankle is not None and right_ankle is not None:
                ankle_y = (left_ankle[1] + right_ankle[1]) / 2
            
            hip_y = left_hip[1] if left_hip is not None else right_hip[1]
            if left_hip is not None and right_hip is not None:
                hip_y = (left_hip[1] + right_hip[1]) / 2
            
            body_center_y = hip_y
            if nose is not None:
                body_center_y = (nose[1] + hip_y) / 2
            
            if ankle_y < body_center_y:  # Ch√¢n cao h∆°n th√¢n
                knee_body_diff = abs(ankle_y - body_center_y) / bbox_height
                if knee_body_diff > thresholds["knee_body_ratio"]:
                    fall_indicators += 1
        
        # 5. Th√™m ki·ªÉm tra g√≥c nghi√™ng c·ªßa th√¢n ng∆∞·ªùi
        if (left_shoulder is not None or right_shoulder is not None) and (left_hip is not None or right_hip is not None):
            shoulder_x = left_shoulder[0] if left_shoulder is not None else right_shoulder[0]
            shoulder_y = left_shoulder[1] if left_shoulder is not None else right_shoulder[1]
            if left_shoulder is not None and right_shoulder is not None:
                shoulder_x = (left_shoulder[0] + right_shoulder[0]) / 2
                shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2
            
            hip_x = left_hip[0] if left_hip is not None else right_hip[0]
            hip_y = left_hip[1] if left_hip is not None else right_hip[1]
            if left_hip is not None and right_hip is not None:
                hip_x = (left_hip[0] + right_hip[0]) / 2
                hip_y = (left_hip[1] + right_hip[1]) / 2
            
            body_angle = abs(np.arctan2(shoulder_y - hip_y, shoulder_x - hip_x))
            # N·∫øu g√≥c nghi√™ng > 45 ƒë·ªô (0.785 radian)
            if body_angle > 0.785:
                fall_indicators += 1
        
        # Quy·∫øt ƒë·ªãnh d·ª±a tr√™n s·ªë l∆∞·ª£ng ch·ªâ s·ªë (c·∫ßn √≠t nh·∫•t 3 ƒëi·ªÉm)
        return fall_indicators >= 3
        
    except Exception as e:
        print(f"‚ùå L·ªói trong analyze_pose_for_fall: {e}")
        return False

def count_people_crossing_line(objects, line_position, frame_width):
    """
    ƒê·∫øm ng∆∞·ªùi qua line gi·ªØa frame v·ªõi ƒë·ªô nh·∫°y cao
    Args:
        objects: Dictionary c·ªßa tracked objects
        line_position: V·ªã tr√≠ x c·ªßa line (gi·ªØa frame)
        frame_width: Chi·ªÅu r·ªông frame
    """
    global people_count_in, people_count_out, total_people, previous_positions
    
    # Debug: In s·ªë l∆∞·ª£ng objects ƒë∆∞·ª£c track
    if len(objects) > 0:
        print(f"üîç Tracking {len(objects)} objects: {list(objects.keys())}")
    
    for object_id, centroid in objects.items():
        # L·∫•y v·ªã tr√≠ hi·ªán t·∫°i
        current_x = centroid[0]
        
        # Debug: In v·ªã tr√≠ c·ªßa t·ª´ng object
        print(f"üìç Object {object_id}: current_x={current_x}, line_position={line_position}")
        
        # Ki·ªÉm tra n·∫øu object ƒë√£ c√≥ v·ªã tr√≠ tr∆∞·ªõc ƒë√≥
        if object_id in previous_positions:
            # L·∫•y v·ªã tr√≠ t·ª´ 30 frame tr∆∞·ªõc (ho·∫∑c frame ƒë·∫ßu ti√™n n·∫øu ch∆∞a ƒë·ªß)
            position_history = previous_positions[object_id]
            if len(position_history) > 0:
                previous_x = position_history[0][0]  # L·∫•y v·ªã tr√≠ x t·ª´ frame c≈© nh·∫•t
                print(f"üìç Object {object_id}: previous_x={previous_x} -> current_x={current_x} (t·ª´ {len(position_history)} frames tr∆∞·ªõc)")
                
                # Logic crossing ƒë∆°n gi·∫£n v√† nh·∫°y h∆°n
                # T·ª´ tr√°i sang ph·∫£i (v√†o)
                if previous_x < line_position and current_x >= line_position:
                    people_count_in += 1
                    total_people += 1
                    print(f"‚úÖ Ng∆∞·ªùi v√†o: ID {object_id}, T·ªïng v√†o: {people_count_in}, T·ªïng hi·ªán t·∫°i: {total_people}")
                    # Reset l·ªãch s·ª≠ sau khi ƒë·∫øm ƒë·ªÉ tr√°nh ƒë·∫øm l·∫∑p
                    previous_positions[object_id] = [centroid]
                
                # T·ª´ ph·∫£i sang tr√°i (ra)
                elif previous_x > line_position and current_x <= line_position:
                    people_count_out += 1
                    total_people = max(0, total_people - 1)  # ƒê·∫£m b·∫£o kh√¥ng √¢m
                    print(f"‚úÖ Ng∆∞·ªùi ra: ID {object_id}, T·ªïng ra: {people_count_out}, T·ªïng hi·ªán t·∫°i: {total_people}")
                    # Reset l·ªãch s·ª≠ sau khi ƒë·∫øm ƒë·ªÉ tr√°nh ƒë·∫øm l·∫∑p
                    previous_positions[object_id] = [centroid]
                else:
                    # Th√™m v·ªã tr√≠ hi·ªán t·∫°i v√†o l·ªãch s·ª≠
                    previous_positions[object_id].append(centroid)
                    # Gi·ªØ t·ªëi ƒëa 30 v·ªã tr√≠ g·∫ßn nh·∫•t (kho·∫£ng 30 frames)
                    if len(previous_positions[object_id]) > 30:
                        previous_positions[object_id].pop(0)
            else:
                # N·∫øu ch∆∞a c√≥ l·ªãch s·ª≠, kh·ªüi t·∫°o
                previous_positions[object_id] = [centroid]
        else:
            print(f"üÜï Object {object_id} m·ªõi xu·∫•t hi·ªán t·∫°i x={current_x}")
            # Kh·ªüi t·∫°o l·ªãch s·ª≠ v·ªã tr√≠ cho object m·ªõi
            previous_positions[object_id] = [centroid]
    
    # X√≥a c√°c object kh√¥ng c√≤n ƒë∆∞·ª£c track
    tracked_ids = set(objects.keys())
    previous_ids = set(previous_positions.keys())
    for old_id in previous_ids - tracked_ids:
        print(f"üóëÔ∏è X√≥a object {old_id} kh·ªèi previous_positions")
        del previous_positions[old_id]

def generate_frames() -> Generator[bytes, None, None]:
    """
    Generator ƒë·ªÉ streaming video frames v·ªõi ThreadedCamera
    """
    global people_count_in, people_count_out, total_people, threaded_camera
    
    # Kh·ªüi t·∫°o threaded camera n·∫øu ch∆∞a c√≥
    if threaded_camera is None:
        try:
            print(f"üîÑ B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o ThreadedCamera...")
            threaded_camera = ThreadedCamera(RTSP_URL)
            print(f"‚è≥ ƒê·ª£i camera ·ªïn ƒë·ªãnh...")
            time.sleep(3)  # ƒê·ª£i camera kh·ªüi t·∫°o v√† ƒë·ªçc frame ƒë·∫ßu ti√™n
            
            # Ki·ªÉm tra camera c√≥ ho·∫°t ƒë·ªông kh√¥ng
            test_ret, test_frame = threaded_camera.read()
            if test_ret and test_frame is not None:
                print(f"‚úÖ ThreadedCamera ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng, frame size: {test_frame.shape}")
            else:
                print(f"‚ö†Ô∏è ThreadedCamera ch∆∞a s·∫µn s√†ng: ret={test_ret}, frame={test_frame is not None}")
                
        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o ThreadedCamera: {e}")
            return
    
    frame_skip_counter = 0
    video_config = config.get("video_config", {})
    frame_skip = video_config.get("frame_skip", 2)
    
    while True:
        try:
            if not hasattr(threaded_camera, 'read'):
                print("‚ö†Ô∏è ThreadedCamera ch∆∞a c√≥ method read")
                time.sleep(0.1)
                continue
                
            ret, frame = threaded_camera.read()
            
            if not ret:
                print(f"‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c frame: ret={ret}")
                time.sleep(0.1)
                continue
                
            if frame is None:
                print("‚ùå Frame l√† None")
                time.sleep(0.1)
                continue
                
            # Debug: In th√¥ng tin frame ƒë·∫ßu ti√™n
            if frame_skip_counter == 0:
                print(f"‚úÖ ƒê·ªçc frame th√†nh c√¥ng: {frame.shape}, dtype: {frame.dtype}")
                
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc frame: {e}")
            time.sleep(0.1)
            continue
        
        # Skip frames ƒë·ªÉ gi·∫£m delay
        frame_skip_counter += 1
        if frame_skip_counter % frame_skip != 0:
            continue
        
        # Ph√°t hi·ªán ng∆∞·ªùi
        rects = detect_people(frame)
        
        # Debug: In s·ªë l∆∞·ª£ng ng∆∞·ªùi ƒë∆∞·ª£c ph√°t hi·ªán
        if len(rects) > 0:
            print(f"üë• Ph√°t hi·ªán {len(rects)} ng∆∞·ªùi: {rects}")
        
        # C·∫≠p nh·∫≠t tracker
        objects = tracker.update(rects)
        
        # Debug: In s·ªë l∆∞·ª£ng objects ƒë∆∞·ª£c track
        if len(objects) > 0:
            print(f"üéØ Tracker ƒëang theo d√µi {len(objects)} objects")
        
        # V·∫Ω line gi·ªØa frame (d·ªçc)
        frame_height, frame_width = frame.shape[:2]
        line_position = frame_width // 2
        ui_config = config.get("ui_config", {})
        line_color = ui_config.get("line_color", [0, 255, 255])
        line_thickness = ui_config.get("line_thickness", 2)
        cv2.line(frame, (line_position, 0), (line_position, frame_height), tuple(line_color), line_thickness)
        
        # ƒê·∫øm ng∆∞·ªùi qua line
        count_people_crossing_line(objects, line_position, frame_width)
        
        # V·∫Ω bounding boxes cho detected people
        ui_config = config.get("ui_config", {})
        bbox_color = tuple(ui_config.get("bbox_color", [0, 255, 0]))
        bbox_thickness = ui_config.get("bbox_thickness", 2)
        text_color = tuple(ui_config.get("text_color", [0, 255, 0]))
        text_scale = ui_config.get("text_scale", 0.5)
        text_thickness = ui_config.get("text_thickness", 2)
        
        for rect in rects:
            (start_x, start_y, end_x, end_y) = rect
            cv2.rectangle(frame, (start_x, start_y), (end_x, end_y), bbox_color, bbox_thickness)
        
        # V·∫Ω centroids v√† IDs
        for (object_id, centroid) in objects.items():
            text = f"ID {object_id}"
            cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, text_scale, text_color, text_thickness)
            cv2.circle(frame, (centroid[0], centroid[1]), 4, text_color, -1)
        
        # Hi·ªÉn th·ªã th√¥ng tin ƒë·∫øm
        info_text = f"Vao: {people_count_in} | Ra: {people_count_out} | Tong: {total_people}"
        cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # Hi·ªÉn th·ªã th√¥ng tin line
        line_text = f"Line dem: X={line_position}"
        cv2.putText(frame, line_text, (10, frame_height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, tuple(line_color), 1)
        
        # Encode frame
        ret, buffer = cv2.imencode('.jpg', frame)
        if ret:
            frame_bytes = buffer.tobytes()
            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
        
        # Gi·∫£m th·ªùi gian sleep ƒë·ªÉ tƒÉng responsiveness
        time.sleep(0.01)  # R·∫•t ng·∫Øn ƒë·ªÉ gi·∫£m delay

def generate_fall_detection_frames() -> Generator[bytes, None, None]:
    """
    Generator ƒë·ªÉ streaming video frames cho fall detection
    """
    global fall_detection_active, threaded_camera
    
    # Kh·ªüi t·∫°o threaded camera n·∫øu ch∆∞a c√≥
    if threaded_camera is None:
        try:
            print(f"üîÑ B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o ThreadedCamera cho fall detection...")
            threaded_camera = ThreadedCamera(RTSP_URL)
            print(f"‚è≥ ƒê·ª£i camera ·ªïn ƒë·ªãnh...")
            time.sleep(3)
        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o ThreadedCamera: {e}")
            return
    
    frame_skip_counter = 0
    frame_skip = 3  # Skip nhi·ªÅu frame h∆°n cho fall detection
    
    while fall_detection_active:
        try:
            ret, frame = threaded_camera.read()
            
            if not ret or frame is None:
                time.sleep(0.1)
                continue
            
            # Skip frames ƒë·ªÉ gi·∫£m delay
            frame_skip_counter += 1
            if frame_skip_counter % frame_skip != 0:
                continue
            
            # Ph√°t hi·ªán poses v√† falls
            poses_data, fall_detected, fall_bbox = detect_poses_and_falls(frame)
            
            # V·∫Ω pose keypoints v√† bounding boxes
            for pose in poses_data:
                bbox = pose['bbox']
                keypoints = pose['keypoints']
                is_fallen = pose['is_fallen']
                confidence = pose['confidence']
                tracker_id = pose.get('tracker_id', 'N/A')
                
                # V·∫Ω bounding box
                color = (0, 0, 255) if is_fallen else (0, 255, 0)  # ƒê·ªè n·∫øu ng√£, xanh n·∫øu b√¨nh th∆∞·ªùng
                thickness = 3 if is_fallen else 2
                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, thickness)
                
                # V·∫Ω label v·ªõi tracker_id
                status = 'FALLEN' if is_fallen else 'NORMAL'
                label = f"ID:{tracker_id} {status} ({confidence:.2f})"
                label_color = (255, 255, 255)
                cv2.putText(frame, label, (bbox[0], bbox[1] - 10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, label_color, 2)
                
                # V·∫Ω keypoints
                for i, kpt in enumerate(keypoints):
                    if kpt['confidence'] > 0.3:
                        x, y = int(kpt['x']), int(kpt['y'])
                        # M√†u kh√°c nhau cho c√°c lo·∫°i keypoints
                        if i in [0]:  # nose
                            kpt_color = (255, 0, 0)  # ƒê·ªè
                        elif i in [5, 6]:  # shoulders
                            kpt_color = (0, 255, 0)  # Xanh l√°
                        elif i in [11, 12]:  # hips
                            kpt_color = (0, 0, 255)  # Xanh d∆∞∆°ng
                        elif i in [15, 16]:  # ankles
                            kpt_color = (255, 255, 0)  # V√†ng
                        else:
                            kpt_color = (255, 255, 255)  # Tr·∫Øng
                        
                        cv2.circle(frame, (x, y), 4, kpt_color, -1)
                
                # V·∫Ω skeleton connections (m·ªôt s·ªë k·∫øt n·ªëi ch√≠nh)
                connections = [
                    (5, 6),   # shoulders
                    (5, 11),  # left shoulder to left hip
                    (6, 12),  # right shoulder to right hip
                    (11, 12), # hips
                    (11, 13), # left hip to left knee
                    (12, 14), # right hip to right knee
                    (13, 15), # left knee to left ankle
                    (14, 16), # right knee to right ankle
                ]
                
                for start_idx, end_idx in connections:
                    start_kpt = keypoints[start_idx]
                    end_kpt = keypoints[end_idx]
                    
                    if start_kpt['confidence'] > 0.3 and end_kpt['confidence'] > 0.3:
                        start_point = (int(start_kpt['x']), int(start_kpt['y']))
                        end_point = (int(end_kpt['x']), int(end_kpt['y']))
                        line_color = (0, 0, 255) if is_fallen else (0, 255, 0)
                        cv2.line(frame, start_point, end_point, line_color, 2)
            
            # Hi·ªÉn th·ªã th√¥ng tin fall detection
            info_text = f"Fall Detection: {'ON' if fall_detection_active else 'OFF'}"
            cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            stats_text = f"People: {people_detected_fall} | Falls: {total_falls}"
            cv2.putText(frame, stats_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            if fall_detected:
                alert_text = "FALL DETECTED!"
                cv2.putText(frame, alert_text, (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)
            
            # Hi·ªÉn th·ªã sensitivity
            sens_text = f"Sensitivity: {fall_sensitivity.upper()}"
            cv2.putText(frame, sens_text, (10, frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # Encode frame
            ret, buffer = cv2.imencode('.jpg', frame)
            if ret:
                frame_bytes = buffer.tobytes()
                yield (b'--frame\r\n'
                       b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
            
            time.sleep(0.02)
            
        except Exception as e:
            print(f"‚ùå L·ªói trong generate_fall_detection_frames: {e}")
            time.sleep(0.1)

@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """
    Endpoint ch√≠nh - hi·ªÉn th·ªã giao di·ªán web
    """
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/api")
async def api_info():
    """
    Endpoint API info
    """
    return {"message": "Camera Tracking API", "status": "running"}

@app.get("/health")
async def health_check():
    """
    Endpoint health check cho Docker
    """
    global threaded_camera, yolo_model
    
    # Ki·ªÉm tra tr·∫°ng th√°i camera
    camera_status = "ok" if threaded_camera and threaded_camera.status else "error"
    
    # Ki·ªÉm tra model
    model_status = "ok" if yolo_model else "error"
    
    # T·ªïng tr·∫°ng th√°i
    overall_status = "healthy" if camera_status == "ok" and model_status == "ok" else "unhealthy"
    
    return {
        "status": overall_status,
        "camera": camera_status,
        "model": model_status,
        "timestamp": time.time()
    }

@app.get("/video_feed")
async def video_feed():
    """
    Endpoint ƒë·ªÉ streaming video
    """
    return StreamingResponse(
        generate_frames(),
        media_type="multipart/x-mixed-replace; boundary=frame"
    )

@app.get("/stats")
async def get_stats():
    """
    Endpoint ƒë·ªÉ l·∫•y th·ªëng k√™ ƒë·∫øm ng∆∞·ªùi
    """
    return {
        "people_in": people_count_in,
        "people_out": people_count_out,
        "total_people": total_people
    }

@app.post("/reset_count")
async def reset_count():
    """
    Reset b·ªô ƒë·∫øm
    """
    global people_count_in, people_count_out, total_people
    people_count_in = 0
    people_count_out = 0
    total_people = 0
    return {"message": "ƒê√£ reset b·ªô ƒë·∫øm"}

# Fall Detection Routes
@app.get("/fall_detection", response_class=HTMLResponse)
async def fall_detection_page(request: Request):
    """
    Trang fall detection
    """
    return templates.TemplateResponse("fall_detection.html", {"request": request})

@app.get("/fall_detection_feed")
async def fall_detection_feed():
    """
    Endpoint ƒë·ªÉ streaming video cho fall detection
    """
    return StreamingResponse(
        generate_fall_detection_frames(),
        media_type="multipart/x-mixed-replace; boundary=frame"
    )

@app.post("/start_fall_detection")
async def start_fall_detection():
    """
    B·∫Øt ƒë·∫ßu fall detection
    """
    global fall_detection_active
    fall_detection_active = True
    return {"message": "ƒê√£ b·∫Øt ƒë·∫ßu fall detection", "status": "active"}

@app.post("/stop_fall_detection")
async def stop_fall_detection():
    """
    D·ª´ng fall detection
    """
    global fall_detection_active
    fall_detection_active = False
    return {"message": "ƒê√£ d·ª´ng fall detection", "status": "inactive"}

@app.get("/fall_detection_stats")
async def get_fall_detection_stats():
    """
    L·∫•y th·ªëng k√™ fall detection
    """
    global total_falls, people_detected_fall, last_fall_time, fall_tracker, last_fall_tracker_id
    
    # Ki·ªÉm tra fall detection g·∫ßn ƒë√¢y (trong 2 gi√¢y)
    recent_fall = False
    if last_fall_time and (time.time() - last_fall_time) < 2:
        recent_fall = True
    
    # Th·ªëng k√™ fall tracker
    active_trackers = len(fall_tracker)
    currently_fallen = sum(1 for data in fall_tracker.values() if data.get('is_fallen', False))
    
    return {
        "total_falls": total_falls,
        "people_detected": people_detected_fall,
        "fall_detected": recent_fall,
        "last_fall_time": last_fall_time,
        "last_fall_tracker_id": last_fall_tracker_id,
        "detection_active": fall_detection_active,
        "sensitivity": fall_sensitivity,
        "confidence_threshold": fall_confidence_threshold,
        "tracker_stats": {
            "active_trackers": active_trackers,
            "currently_fallen": currently_fallen,
            "cooldown_time": fall_cooldown_time,
            "max_duration": max_fall_duration
        },
        "pose_data": {
            "keypoints": []  # C√≥ th·ªÉ m·ªü r·ªông ƒë·ªÉ tr·∫£ v·ªÅ pose data chi ti·∫øt
        }
    }

@app.post("/reset_fall_stats")
async def reset_fall_stats():
    """
    Reset th·ªëng k√™ fall detection v√† fall tracker
    """
    global total_falls, people_detected_fall, last_fall_time, fall_tracker, last_fall_tracker_id
    total_falls = 0
    people_detected_fall = 0
    last_fall_time = None
    last_fall_tracker_id = None
    fall_tracker.clear()  # X√≥a t·∫•t c·∫£ tracking data
    return {
        "message": "ƒê√£ reset th·ªëng k√™ fall detection v√† fall tracker",
        "total_falls": total_falls,
        "active_trackers": len(fall_tracker)
    }

@app.post("/update_fall_sensitivity")
async def update_fall_sensitivity(request: Request):
    """
    C·∫≠p nh·∫≠t ƒë·ªô nh·∫°y fall detection
    """
    global fall_sensitivity
    try:
        data = await request.json()
        sensitivity = data.get("sensitivity", "medium")
        if sensitivity in ["low", "medium", "high"]:
            fall_sensitivity = sensitivity
            return {"message": f"ƒê√£ c·∫≠p nh·∫≠t sensitivity: {sensitivity}", "sensitivity": fall_sensitivity}
        else:
            return {"error": "Sensitivity kh√¥ng h·ª£p l·ªá"}, 400
    except Exception as e:
        return {"error": str(e)}, 400

@app.post("/update_fall_confidence")
async def update_fall_confidence(request: Request):
    """
    C·∫≠p nh·∫≠t ng∆∞·ª°ng tin c·∫≠y fall detection
    """
    global fall_confidence_threshold
    try:
        data = await request.json()
        confidence = data.get("confidence", 0.6)
        if 0.3 <= confidence <= 0.9:
            fall_confidence_threshold = confidence
            return {"message": f"ƒê√£ c·∫≠p nh·∫≠t confidence: {confidence}", "confidence": fall_confidence_threshold}
        else:
            return {"error": "Confidence threshold ph·∫£i t·ª´ 0.3 ƒë·∫øn 0.9"}, 400
    except Exception as e:
        return {"error": str(e)}, 400

if __name__ == "__main__":
    server_config = config.get("server_config", {"host": "0.0.0.0", "port": 8000})
    host = server_config.get("host", "0.0.0.0")
    port = server_config.get("port", 8000)
    
    print(f"üöÄ Kh·ªüi ƒë·ªông Camera Tracking Server...")
    print(f"üì° RTSP URL: {RTSP_URL}")
    print(f"üåê Server: http://{host}:{port}")
    print(f"üìä Confidence threshold: {CONFIDENCE_THRESHOLD}")
    print(f"üéØ Tracker config: {tracker_config}")
    
    uvicorn.run(app, host=host, port=port)